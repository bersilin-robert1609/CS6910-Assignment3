{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "random.seed()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: \"<\", EOS_token: \">\"}\n",
    "        self.n_chars = 2  # Count SOS and EOS\n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.word2index:\n",
    "            self.word2index[char] = self.n_chars\n",
    "            self.word2count[char] = 1\n",
    "            self.index2word[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.word2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_output_data(lang: str, type:str):\n",
    "    path = \"./aksharantar_sampled/{}/{}_{}.csv\".format(lang, lang, type)\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    return df[0].to_numpy(), df[1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_words(lang:str):\n",
    "    input_lang, output_lang = Language('eng'), Language(lang)\n",
    "    input_words, output_words = input_output_data(lang, 'train')\n",
    "    word_pairs = [[input_words[i], output_words[i]] for i in range(len(input_words))]\n",
    "    for word in input_words:\n",
    "        input_lang.addWord(word)\n",
    "    for word in output_words:\n",
    "        output_lang.addWord(word)\n",
    "    return input_lang, output_lang, word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['puliyurai', 'புலியூரை']\n",
      "Number of words in input language:  51200\n",
      "Number of characters in input language:  28\n",
      "Number of characters in output language:  48\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = set_words('tam')\n",
    "print(random.choice(pairs))\n",
    "print(\"Number of words in input language: \", len(pairs))\n",
    "print(\"Number of characters in input language: \", input_lang.n_chars)\n",
    "print(\"Number of characters in output language: \", output_lang.n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded output:  [2, 8, 5, 10, 7, 20, 5, 4, 17, 21, 13]\n",
      "Decoded output:  ['த', 'ர', '்', 'ம', 'ச', 'ண', '்', 'ட', 'ி', 'க', 'ை']\n",
      "Decoded string:  தர்மசண்டிகை\n"
     ]
    }
   ],
   "source": [
    "random_pair = random.choice(pairs)\n",
    "input_word = random_pair[0]\n",
    "output_word = random_pair[1]\n",
    "\n",
    "encoded_output = [output_lang.word2index[char] for char in output_word]\n",
    "print(\"Encoded output: \", encoded_output)\n",
    "\n",
    "decoded_output = [output_lang.index2word[i] for i in encoded_output]\n",
    "print(\"Decoded output: \", decoded_output)\n",
    "\n",
    "decoded_string = ''.join(decoded_output)\n",
    "print(\"Decoded string: \", decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell(cell_type:str):\n",
    "    if cell_type == 'LSTM':\n",
    "        return nn.LSTM\n",
    "    elif cell_type == 'GRU':\n",
    "        return nn.GRU\n",
    "    elif cell_type == 'RNN':\n",
    "        return nn.RNN\n",
    "    else:\n",
    "        raise Exception(\"Invalid cell type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64\n",
    "INPUT_DIM = input_lang.n_chars\n",
    "OUTPUT_DIM = output_lang.n_chars\n",
    "HIDDEN_DIM = 256\n",
    "CELL_TYPE = 'GRU'\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "PRINT_EVERY = 1000\n",
    "PLOT_EVERY = 100\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        self.embedding = nn.Embedding(INPUT_DIM, EMBED_DIM)\n",
    "\n",
    "        # cell types = \"RNN\", \"GRU\", \"LSTM\"\n",
    "        self.cell = cell(CELL_TYPE)(EMBED_DIM, HIDDEN_DIM)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.cell(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        self.embedding = nn.Embedding(OUTPUT_DIM, EMBED_DIM)\n",
    "\n",
    "        # cell types = \"RNN\", \"GRU\", \"LSTM\"\n",
    "        self.cell = cell(CELL_TYPE)(EMBED_DIM, HIDDEN_DIM)\n",
    "        self.out = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embed = self.embedding(input).view(1, 1, -1)\n",
    "        active_embed = F.relu(embed)\n",
    "        output, hidden = self.cell(active_embed, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromWord(lang:Language, word:str):\n",
    "    return [lang.word2index[char] for char in word]\n",
    "\n",
    "def tensorFromWord(lang:Language, word:str):\n",
    "    indexes = indexesFromWord(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair:list):\n",
    "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
    "    target_tensor = tensorFromWord(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, \n",
    "          target_tensor, \n",
    "          encoder : EncoderRNN,\n",
    "          decoder : DecoderRNN,\n",
    "          encoder_optimizer : optim.Optimizer, \n",
    "          decoder_optimizer : optim.Optimizer,\n",
    "          criterion,\n",
    "          max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_dim, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs size:  51200\n"
     ]
    }
   ],
   "source": [
    "training_pairs = [tensorsFromPair(pair) for pair in pairs]\n",
    "print(\"Training pairs size: \", len(training_pairs))\n",
    "\n",
    "def train_loop(encoder : EncoderRNN,\n",
    "               decoder : DecoderRNN,\n",
    "               n_iters : int = 5,\n",
    "               print_every=PRINT_EVERY, \n",
    "               plot_every=PLOT_EVERY,\n",
    "               learning_rate=LEARNING_RATE):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # random permutate the training pairs\n",
    "    random.shuffle(training_pairs)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, len(training_pairs)+1):\n",
    "        training_pair = training_pairs[iter-1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            current_time = time.time()\n",
    "            print(\"Loss after {} iterations ({}s): {}\".format(iter, current_time - start_time, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder : EncoderRNN,\n",
    "             decoder : DecoderRNN,\n",
    "             word : str,\n",
    "             max_length=MAX_LENGTH):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromWord(input_lang, word)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_dim, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_word = \"\"\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "            else:\n",
    "                decoded_word += output_lang.index2word[topi.item()]\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random(encoder:EncoderRNN, decoder:DecoderRNN, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print(\"Input: {}\".format(pair[0]))\n",
    "        print(\"Target: {}\".format(pair[1]))\n",
    "        output = evaluate(encoder, decoder, pair[0])\n",
    "        print(\"Output: {}\".format(output))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loss after 5000 iterations (32.570534229278564s): 2.761272708293739\n",
      "Loss after 10000 iterations (64.56866693496704s): 2.576452040883524\n",
      "Loss after 15000 iterations (96.51885867118835s): 2.5351037977381115\n",
      "Loss after 20000 iterations (128.96645998954773s): 2.5098527596083904\n",
      "Loss after 25000 iterations (161.5207736492157s): 2.4745074851416615\n",
      "Loss after 30000 iterations (194.34914422035217s): 2.454995440347155\n",
      "Loss after 35000 iterations (227.49340748786926s): 2.430090384004403\n",
      "Loss after 40000 iterations (264.2547187805176s): 2.3978343262346424\n",
      "Loss after 45000 iterations (298.7650682926178s): 2.3538481898292436\n",
      "Loss after 50000 iterations (333.62473917007446s): 2.316358998466944\n",
      "Epoch 1\n",
      "Loss after 5000 iterations (35.29433274269104s): 2.2542203995663725\n",
      "Loss after 10000 iterations (70.68840432167053s): 2.1761930697732526\n",
      "Loss after 15000 iterations (106.17613887786865s): 2.1479104773283786\n",
      "Loss after 20000 iterations (142.5713927745819s): 2.0786312474793003\n",
      "Loss after 25000 iterations (178.7797667980194s): 2.022957819072438\n",
      "Loss after 30000 iterations (213.95763969421387s): 1.9415436399755508\n",
      "Loss after 35000 iterations (248.8397240638733s): 1.8679490169130768\n",
      "Loss after 40000 iterations (283.70409631729126s): 1.7784570948636982\n",
      "Loss after 45000 iterations (318.6387891769409s): 1.6869351867480225\n",
      "Loss after 50000 iterations (353.5624613761902s): 1.6127935494727503\n",
      "Epoch 2\n",
      "Loss after 5000 iterations (36.07794737815857s): 1.5329683186234542\n",
      "Loss after 10000 iterations (71.99230122566223s): 1.4222193063509023\n",
      "Loss after 15000 iterations (107.92308855056763s): 1.3561611007490315\n",
      "Loss after 20000 iterations (145.84311723709106s): 1.299238376550867\n",
      "Loss after 25000 iterations (183.58548307418823s): 1.2421682344890521\n",
      "Loss after 30000 iterations (219.77406454086304s): 1.1757782597978517\n",
      "Loss after 35000 iterations (255.50415754318237s): 1.1319650171967675\n",
      "Loss after 40000 iterations (290.626668214798s): 1.0572843880725273\n",
      "Loss after 45000 iterations (325.0210590362549s): 0.9996682458211433\n",
      "Loss after 50000 iterations (358.7662320137024s): 0.9791809522221072\n",
      "Epoch 3\n",
      "Loss after 5000 iterations (33.93781495094299s): 0.9041347380261705\n",
      "Loss after 10000 iterations (67.61525058746338s): 0.8703461307561909\n",
      "Loss after 15000 iterations (101.44062876701355s): 0.8118473226255366\n",
      "Loss after 20000 iterations (135.2222695350647s): 0.779726085720451\n",
      "Loss after 25000 iterations (169.11634278297424s): 0.7585946794327507\n",
      "Loss after 30000 iterations (203.4999852180481s): 0.7514025823034576\n",
      "Loss after 35000 iterations (243.48074102401733s): 0.7098171804950152\n",
      "Loss after 40000 iterations (287.91856575012207s): 0.7004711493872322\n",
      "Loss after 45000 iterations (333.0461127758026s): 0.6584565027740369\n",
      "Loss after 50000 iterations (378.4760615825653s): 0.6410793000442427\n",
      "Epoch 4\n",
      "Loss after 5000 iterations (45.50424408912659s): 0.621476173626038\n",
      "Loss after 10000 iterations (89.40641784667969s): 0.5974060716801067\n",
      "Loss after 15000 iterations (131.70495438575745s): 0.6048841802429322\n",
      "Loss after 20000 iterations (173.92585015296936s): 0.5780230316994605\n",
      "Loss after 25000 iterations (216.18018913269043s): 0.5376271260062776\n",
      "Loss after 30000 iterations (258.94633746147156s): 0.5503501639693039\n",
      "Loss after 35000 iterations (301.33197951316833s): 0.5140783391513488\n",
      "Loss after 40000 iterations (344.4316544532776s): 0.524726259322129\n",
      "Loss after 45000 iterations (386.3589470386505s): 0.5085168850776253\n",
      "Loss after 50000 iterations (428.5617756843567s): 0.508389317956695\n",
      "Epoch 5\n",
      "Loss after 5000 iterations (41.52247905731201s): 0.4661995847019781\n",
      "Loss after 10000 iterations (83.18355298042297s): 0.459930533738167\n",
      "Loss after 15000 iterations (125.10369992256165s): 0.447275445565936\n",
      "Loss after 20000 iterations (160.2367627620697s): 0.45120125121088833\n",
      "Loss after 25000 iterations (195.19252562522888s): 0.4467929811356163\n",
      "Loss after 30000 iterations (234.47407793998718s): 0.42685745933568287\n",
      "Loss after 35000 iterations (273.77813601493835s): 0.44394220801011464\n",
      "Loss after 40000 iterations (314.49972438812256s): 0.44166575399626806\n",
      "Loss after 45000 iterations (354.615802526474s): 0.42109987786817155\n",
      "Loss after 50000 iterations (396.74102449417114s): 0.4294176342807488\n",
      "Epoch 6\n",
      "Loss after 5000 iterations (40.57685041427612s): 0.38944966150690774\n",
      "Loss after 10000 iterations (75.4860029220581s): 0.3837830496516279\n",
      "Loss after 15000 iterations (110.58538675308228s): 0.3814677082250102\n",
      "Loss after 20000 iterations (144.9321587085724s): 0.3766688724224547\n",
      "Loss after 25000 iterations (179.12706017494202s): 0.37914255827290305\n",
      "Loss after 30000 iterations (213.29454135894775s): 0.37077112408578383\n",
      "Loss after 35000 iterations (247.45392727851868s): 0.3562087327017764\n",
      "Loss after 40000 iterations (281.6662709712982s): 0.3752910953039704\n",
      "Loss after 45000 iterations (315.8363609313965s): 0.3687676700333119\n",
      "Loss after 50000 iterations (350.1127607822418s): 0.3601618192047379\n",
      "Epoch 7\n",
      "Loss after 5000 iterations (34.330745220184326s): 0.33400645717417315\n",
      "Loss after 10000 iterations (68.4532778263092s): 0.34143508669917244\n",
      "Loss after 15000 iterations (102.89071321487427s): 0.33151784498390663\n",
      "Loss after 20000 iterations (137.15677738189697s): 0.3354146362968368\n",
      "Loss after 25000 iterations (171.34978246688843s): 0.3160036104485383\n",
      "Loss after 30000 iterations (205.3452923297882s): 0.3265545759300889\n",
      "Loss after 35000 iterations (239.59132885932922s): 0.3505108304958603\n",
      "Loss after 40000 iterations (273.9737982749939s): 0.30585361594279636\n",
      "Loss after 45000 iterations (308.0071885585785s): 0.31832469987850376\n",
      "Loss after 50000 iterations (342.2877571582794s): 0.3368762875331487\n",
      "Epoch 8\n",
      "Loss after 5000 iterations (34.35132074356079s): 0.28610513768844087\n",
      "Loss after 10000 iterations (68.64664196968079s): 0.2782664465741252\n",
      "Loss after 15000 iterations (102.89111351966858s): 0.30300284655631116\n",
      "Loss after 20000 iterations (137.02739906311035s): 0.2990227536143735\n",
      "Loss after 25000 iterations (171.23995065689087s): 0.29444842308578933\n",
      "Loss after 30000 iterations (205.54606294631958s): 0.29514182287737356\n",
      "Loss after 35000 iterations (239.90413403511047s): 0.2958439768229659\n",
      "Loss after 40000 iterations (274.19518518447876s): 0.306160862259487\n",
      "Loss after 45000 iterations (308.5338439941406s): 0.30281513799359344\n",
      "Loss after 50000 iterations (342.74974942207336s): 0.29677555439271436\n",
      "Epoch 9\n",
      "Loss after 5000 iterations (34.291584968566895s): 0.2742420727488042\n",
      "Loss after 10000 iterations (68.4966139793396s): 0.2719389655684199\n",
      "Loss after 15000 iterations (102.83304786682129s): 0.2549534601311674\n",
      "Loss after 20000 iterations (137.19546580314636s): 0.2678613553615169\n",
      "Loss after 25000 iterations (171.2315227985382s): 0.2701105297924671\n",
      "Loss after 30000 iterations (205.5449938774109s): 0.2679502359432259\n",
      "Loss after 35000 iterations (239.92334914207458s): 0.28349257732240324\n",
      "Loss after 40000 iterations (274.155748128891s): 0.27777766923871366\n",
      "Loss after 45000 iterations (308.30304527282715s): 0.2619109067973575\n",
      "Loss after 50000 iterations (342.40345764160156s): 0.2826419652953179\n"
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN().to(device)\n",
    "decoder1 = DecoderRNN().to(device)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    train_loop(encoder1, decoder1, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), \"./models/encoder_tam1.pt\")\n",
    "torch.save(decoder1.state_dict(), \"./models/decoder_tam1.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
