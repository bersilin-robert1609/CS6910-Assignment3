{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs20b013\u001b[0m (\u001b[33mcs20b013-bersilin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "random.seed()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language Model\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: \"<\", EOS_token: \">\"}\n",
    "        self.n_chars = 2  # Count SOS and EOS\n",
    "\n",
    "    def addWord(self, word):\n",
    "        for char in word:\n",
    "            self.addChar(char)\n",
    "\n",
    "    def addChar(self, char):\n",
    "        if char not in self.word2index:\n",
    "            self.word2index[char] = self.n_chars\n",
    "            self.word2count[char] = 1\n",
    "            self.index2word[self.n_chars] = char\n",
    "            self.n_chars += 1\n",
    "        else:\n",
    "            self.word2count[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(lang: str, type: str) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Returns: 'pairs': list of [input_word, target_word] pairs\n",
    "    \"\"\"\n",
    "    path = \"./aksharantar_sampled/{}/{}_{}.csv\".format(lang, lang, type)\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    pairs = df.values.tolist()\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages(lang: str):\n",
    "    \"\"\"\n",
    "    Returns \n",
    "    1. input_lang: input language - English\n",
    "    2. output_lang: output language - Given language\n",
    "    3. pairs: list of [input_word, target_word] pairs\n",
    "    \"\"\"\n",
    "    input_lang = Language('eng')\n",
    "    output_lang = Language(lang)\n",
    "    pairs = get_data(lang, \"train\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addWord(pair[0])\n",
    "        output_lang.addWord(pair[1])\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell(cell_type: str):\n",
    "    if cell_type == \"LSTM\":\n",
    "        return nn.LSTM\n",
    "    elif cell_type == \"GRU\":\n",
    "        return nn.GRU\n",
    "    elif cell_type == \"RNN\":\n",
    "        return nn.RNN\n",
    "    else:\n",
    "        raise Exception(\"Invalid cell type\")\n",
    "    \n",
    "def get_optimizer(optimizer: str):\n",
    "    if optimizer == \"SGD\":\n",
    "        return optim.SGD\n",
    "    elif optimizer == \"ADAM\":\n",
    "        return optim.Adam\n",
    "    else:\n",
    "        raise Exception(\"Invalid optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_sz: int,\n",
    "                 embed_sz: int,\n",
    "                 hidden_sz: int,\n",
    "                 cell_type: str,\n",
    "                 n_layers: int,\n",
    "                 dropout: float):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.cell_type = cell_type\n",
    "        self.embedding = nn.Embedding(in_sz, embed_sz)\n",
    "\n",
    "        self.rnn = get_cell(cell_type)(input_size = embed_sz,\n",
    "                                       hidden_size = hidden_sz,\n",
    "                                       num_layers = n_layers,\n",
    "                                       dropout = dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        if(self.cell_type == \"LSTM\"):\n",
    "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded, hidden)\n",
    "            \n",
    "        return output, hidden, cell\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_sz, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 out_sz: int,\n",
    "                 embed_sz: int,\n",
    "                 hidden_sz: int,\n",
    "                 cell_type: str,\n",
    "                 n_layers: int,\n",
    "                 dropout: float):\n",
    "\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.cell_type = cell_type\n",
    "        self.embedding = nn.Embedding(out_sz, embed_sz)\n",
    "\n",
    "        self.attn = nn.Linear(hidden_sz + embed_sz, 50)\n",
    "        self.attn_combine = nn.Linear(hidden_sz + embed_sz, hidden_sz)\n",
    "\n",
    "        self.rnn = get_cell(cell_type)(input_size = hidden_sz,\n",
    "                                       hidden_size = hidden_sz,\n",
    "                                       num_layers = n_layers,\n",
    "                                       dropout = dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_sz, out_sz)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        embedding = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedding[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedding[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        if(self.cell_type == \"LSTM\"):\n",
    "            output, (hidden, cell) = self.rnn(output, (hidden, cell))\n",
    "        else:\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, cell, attn_weights\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_sz, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromWord(lang:Language, word:str):\n",
    "    return [lang.word2index[char] for char in word]\n",
    "\n",
    "def tensorFromWord(lang:Language, word:str):\n",
    "    indexes = indexesFromWord(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(input_lang:Language, output_lang:Language, pair:list[str]):\n",
    "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
    "    target_tensor = tensorFromWord(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_definition(): \n",
    "    \"\"\"\n",
    "    params:\n",
    "\n",
    "        embed_size : size of embedding (input and output) (8, 16, 32, 64)\n",
    "        hidden_size : size of hidden layer (64, 128, 256, 512)\n",
    "        cell_type : type of cell (LSTM, GRU, RNN)\n",
    "        num_layers : number of layers in encoder (1, 2, 3)\n",
    "        dropout : dropout probability\n",
    "        learning_rate : learning rate\n",
    "        teacher_forcing_ratio : teacher forcing ratio (0.5 fixed for now)\n",
    "        optimizer : optimizer (SGD, Adam)\n",
    "        max_length : maximum length of input word (50 fixed for now)\n",
    "\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_EVERY = 5000\n",
    "PLOT_EVERY = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    def __init__(self, lang: str, params: dict):\n",
    "        self.lang = lang\n",
    "        self.input_lang, self.output_lang, self.pairs = get_languages(self.lang)\n",
    "        self.input_size = self.input_lang.n_chars\n",
    "        self.output_size = self.output_lang.n_chars\n",
    "\n",
    "        self.training_pairs = [tensorsFromPair(self.input_lang, self.output_lang, pair) for pair in self.pairs]\n",
    "\n",
    "        self.encoder = Encoder(in_sz = self.input_size,\n",
    "                             embed_sz = params[\"embed_size\"],\n",
    "                             hidden_sz = params[\"hidden_size\"],\n",
    "                             cell_type = params[\"cell_type\"],\n",
    "                             n_layers = params[\"num_layers\"],\n",
    "                             dropout = params[\"dropout\"]).to(device)\n",
    "        \n",
    "        self.decoder = AttentionDecoder(out_sz = self.output_size,\n",
    "                             embed_sz = params[\"embed_size\"],\n",
    "                             hidden_sz = params[\"hidden_size\"],\n",
    "                             cell_type = params[\"cell_type\"],\n",
    "                             n_layers = params[\"num_layers\"],\n",
    "                             dropout = params[\"dropout\"]).to(device)\n",
    "\n",
    "        self.encoder_optimizer = get_optimizer(params[\"optimizer\"])(self.encoder.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
    "        self.decoder_optimizer = get_optimizer(params[\"optimizer\"])(self.decoder.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
    "        \n",
    "        self.criterion = nn.NLLLoss()\n",
    "\n",
    "        self.teacher_forcing_ratio = params[\"teacher_forcing_ratio\"]\n",
    "        self.max_length = params[\"max_length\"]\n",
    "\n",
    "    def train_single(self, input_tensor, target_tensor):\n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_cell = self.encoder.initHidden()\n",
    "\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(self.max_length, self.encoder.hidden_sz, device=device)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "                decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "\n",
    "        loss.backward()\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "\n",
    "        return loss.item() / target_length\n",
    "    \n",
    "    def train(self, iters=-1):\n",
    "        start_time = time.time()\n",
    "        plot_losses = []\n",
    "        print_loss_total = 0\n",
    "        plot_loss_total = 0\n",
    "\n",
    "        random.shuffle(self.training_pairs)\n",
    "        iters = len(self.training_pairs) if iters == -1 else iters\n",
    "\n",
    "        for iter in range(1, iters):\n",
    "            training_pair = self.training_pairs[iter - 1]\n",
    "            input_tensor = training_pair[0]\n",
    "            target_tensor = training_pair[1]\n",
    "\n",
    "            loss = self.train_single(input_tensor, target_tensor)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if iter % PRINT_EVERY == 0:\n",
    "                print_loss_avg = print_loss_total / PRINT_EVERY\n",
    "                print_loss_total = 0\n",
    "                current_time = time.time()\n",
    "                print(\"Loss: {:.4f} | Iterations: {} | Time: {:.3f}\".format(print_loss_avg, iter, current_time - start_time))\n",
    "\n",
    "            if iter % PLOT_EVERY == 0:\n",
    "                plot_loss_avg = plot_loss_total / PLOT_EVERY\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "            \n",
    "        return plot_losses\n",
    "    \n",
    "    def evaluate(self, word):\n",
    "        with torch.no_grad():\n",
    "            input_tensor = tensorFromWord(self.input_lang, word)\n",
    "            input_length = input_tensor.size()[0]\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "            encoder_cell = self.encoder.initHidden()\n",
    "\n",
    "            encoder_outputs = torch.zeros(self.max_length, self.encoder.hidden_sz, device=device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "                encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "            decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "\n",
    "            decoded_chars = \"\"\n",
    "            decoder_attentions = torch.zeros(self.max_length, self.max_length)\n",
    "\n",
    "            for di in range(self.max_length):\n",
    "                decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                decoder_attentions[di] = decoder_attention.data\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                \n",
    "                if topi.item() == EOS_token:\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_chars += self.output_lang.index2word[topi.item()]\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            return decoded_chars, decoder_attentions[:di + 1]\n",
    "        \n",
    "    def test_validate(self, type:str):\n",
    "        pairs = get_data(self.lang, type)\n",
    "        accuracy = 0\n",
    "        for pair in pairs:\n",
    "            output, _ = self.evaluate(pair[0])\n",
    "            if output == pair[1]:\n",
    "                accuracy += 1\n",
    "        return accuracy / len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(plot_losses, title: str):\n",
    "    # return plot of losses\n",
    "    x_labels = [i * PLOT_EVERY for i in range(1, len(plot_losses) + 1)]\n",
    "    plt.plot(x_labels, plot_losses, color=\"blue\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"validation_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"embed_size\": {\n",
    "            \"values\": [8, 16, 32]\n",
    "        },\n",
    "        \"hidden_size\": {\n",
    "            \"values\": [64, 128, 256]\n",
    "        },\n",
    "        \"cell_type\": {\n",
    "            \"values\": [\"GRU\"]\n",
    "        },\n",
    "        \"num_layers\": {\n",
    "            \"values\": [1, 2]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0, 0.1, 0.2]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.0005, 0.001, 0.005]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"SGD\", \"ADAM\"]\n",
    "        },\n",
    "        \"teacher_forcing_ratio\": {\n",
    "            'value': 0.5\n",
    "        },\n",
    "        \"max_length\": {\n",
    "            'value': 50\n",
    "        },\n",
    "        \"weight_decay\": {\n",
    "            \"values\": [0, 1e-1, 1e-3, 1e-5]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "def train_sweep():\n",
    "    global count\n",
    "    count += 1\n",
    "\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "    run.name = \"embed_size: {} | hidden_size: {} | cell_type: {} | num_layers: {} | dropout: {} | learning_rate: {} | optimizer: {} | teacher_forcing_ratio: {} | max_length: {} | weight_decay: {}\".format(config.embed_size, config.hidden_size, config.cell_type, config.num_layers, config.dropout, config.learning_rate, config.optimizer, config.teacher_forcing_ratio, config.max_length, config.weight_decay)\n",
    "\n",
    "    model = Translator('tam', config)\n",
    "\n",
    "    epochs = 10\n",
    "    old_validation_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: {}\".format(epoch + 1))\n",
    "        plot_losses = model.train()\n",
    "\n",
    "        # take average of plot losses as training loss\n",
    "        training_loss = sum(plot_losses) / len(plot_losses)\n",
    "        \n",
    "        training_accuracy = model.test_validate('train')\n",
    "        print(\"Training Accuracy: {:.4f}\".format(training_accuracy))\n",
    "\n",
    "        validation_accuracy = model.test_validate('valid')\n",
    "        print(\"Validation Accuracy: {:.4f}\".format(validation_accuracy))\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"training_loss\": training_loss,\n",
    "            \"training_accuracy\": training_accuracy,\n",
    "            \"validation_accuracy\": validation_accuracy\n",
    "        })\n",
    "\n",
    "        if epoch > 0:\n",
    "            if validation_accuracy < 0.0001:\n",
    "                break\n",
    "\n",
    "            if validation_accuracy < 0.9 * old_validation_accuracy:\n",
    "                break\n",
    "\n",
    "        old_validation_accuracy = validation_accuracy\n",
    "\n",
    "    test_accuracy = model.test_validate('test')\n",
    "    print(\"Test Accuracy: {:.4f}\".format(test_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"test_accuracy\": test_accuracy\n",
    "    })\n",
    "\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6t2a5sx4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bersi/Documents/CS6910 Assignments/CS6910-Assignment3/wandb/run-20230508_065858-6t2a5sx4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/6t2a5sx4' target=\"_blank\">eager-sweep-15</a></strong> to <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/6t2a5sx4' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/6t2a5sx4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 2.5801 | Iterations: 5000 | Time: 87.542\n",
      "Loss: 1.9078 | Iterations: 10000 | Time: 174.953\n",
      "Loss: 2.1354 | Iterations: 15000 | Time: 262.277\n",
      "Loss: 2.6954 | Iterations: 20000 | Time: 348.662\n",
      "Loss: 2.6108 | Iterations: 25000 | Time: 435.683\n",
      "Loss: 2.5809 | Iterations: 30000 | Time: 522.485\n",
      "Loss: 2.5445 | Iterations: 35000 | Time: 608.893\n",
      "Loss: 2.5419 | Iterations: 40000 | Time: 696.629\n",
      "Loss: 2.6272 | Iterations: 45000 | Time: 776.224\n",
      "Loss: 2.7122 | Iterations: 50000 | Time: 845.783\n",
      "Training Accuracy: 0.0000\n",
      "Validation Accuracy: 0.0000\n",
      "Epoch: 2\n",
      "Loss: 2.6661 | Iterations: 5000 | Time: 83.565\n",
      "Loss: 2.6619 | Iterations: 10000 | Time: 167.391\n",
      "Loss: 2.6431 | Iterations: 15000 | Time: 250.754\n",
      "Loss: 2.6425 | Iterations: 20000 | Time: 334.457\n",
      "Loss: 2.6374 | Iterations: 25000 | Time: 417.425\n",
      "Loss: 2.6161 | Iterations: 30000 | Time: 500.968\n",
      "Loss: 2.6177 | Iterations: 35000 | Time: 584.650\n",
      "Loss: 2.6160 | Iterations: 40000 | Time: 668.976\n",
      "Loss: 2.6120 | Iterations: 45000 | Time: 739.344\n",
      "Loss: 2.5902 | Iterations: 50000 | Time: 805.705\n",
      "Training Accuracy: 0.0000\n",
      "Validation Accuracy: 0.0000\n",
      "Test Accuracy: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_accuracy</td><td>▁▁</td></tr><tr><td>training_loss</td><td>▁█</td></tr><tr><td>validation_accuracy</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_accuracy</td><td>0.0</td></tr><tr><td>training_accuracy</td><td>0.0</td></tr><tr><td>training_loss</td><td>2.62917</td></tr><tr><td>validation_accuracy</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-15</strong> at: <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/6t2a5sx4' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/6t2a5sx4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_065858-6t2a5sx4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uwqzss76 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bersi/Documents/CS6910 Assignments/CS6910-Assignment3/wandb/run-20230508_074604-uwqzss76</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/uwqzss76' target=\"_blank\">light-sweep-16</a></strong> to <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/uwqzss76' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/uwqzss76</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 2.6586 | Iterations: 5000 | Time: 76.831\n",
      "Loss: 2.4853 | Iterations: 10000 | Time: 153.086\n",
      "Loss: 1.8829 | Iterations: 15000 | Time: 229.452\n",
      "Loss: 1.1744 | Iterations: 20000 | Time: 303.334\n",
      "Loss: 0.8546 | Iterations: 25000 | Time: 378.355\n",
      "Loss: 0.6216 | Iterations: 30000 | Time: 454.422\n",
      "Loss: 0.5186 | Iterations: 35000 | Time: 533.546\n",
      "Loss: 0.4560 | Iterations: 40000 | Time: 613.745\n",
      "Loss: 0.3943 | Iterations: 45000 | Time: 688.741\n",
      "Loss: 0.3977 | Iterations: 50000 | Time: 751.847\n",
      "Training Accuracy: 0.5679\n",
      "Validation Accuracy: 0.4756\n",
      "Epoch: 2\n",
      "Loss: 0.3328 | Iterations: 5000 | Time: 67.729\n",
      "Loss: 0.3594 | Iterations: 10000 | Time: 134.080\n",
      "Loss: 0.3218 | Iterations: 15000 | Time: 208.308\n",
      "Loss: 0.3120 | Iterations: 20000 | Time: 275.685\n",
      "Loss: 0.3034 | Iterations: 25000 | Time: 342.345\n",
      "Loss: 0.2862 | Iterations: 30000 | Time: 409.378\n",
      "Loss: 0.2782 | Iterations: 35000 | Time: 476.988\n",
      "Loss: 0.2882 | Iterations: 40000 | Time: 544.091\n",
      "Loss: 0.2923 | Iterations: 45000 | Time: 611.139\n",
      "Loss: 0.2687 | Iterations: 50000 | Time: 673.995\n",
      "Training Accuracy: 0.6651\n",
      "Validation Accuracy: 0.5349\n",
      "Epoch: 3\n",
      "Loss: 0.2536 | Iterations: 5000 | Time: 66.622\n",
      "Loss: 0.2583 | Iterations: 10000 | Time: 133.454\n",
      "Loss: 0.2587 | Iterations: 15000 | Time: 199.929\n",
      "Loss: 0.2535 | Iterations: 20000 | Time: 266.390\n",
      "Loss: 0.2594 | Iterations: 25000 | Time: 333.037\n",
      "Loss: 0.2580 | Iterations: 30000 | Time: 399.624\n",
      "Loss: 0.2475 | Iterations: 35000 | Time: 466.417\n",
      "Loss: 0.2357 | Iterations: 40000 | Time: 533.359\n",
      "Loss: 0.2298 | Iterations: 45000 | Time: 599.941\n",
      "Loss: 0.2498 | Iterations: 50000 | Time: 664.267\n",
      "Training Accuracy: 0.6973\n",
      "Validation Accuracy: 0.5542\n",
      "Epoch: 4\n",
      "Loss: 0.2184 | Iterations: 5000 | Time: 66.672\n",
      "Loss: 0.2148 | Iterations: 10000 | Time: 133.340\n",
      "Loss: 0.2109 | Iterations: 15000 | Time: 200.066\n",
      "Loss: 0.2057 | Iterations: 20000 | Time: 267.195\n",
      "Loss: 0.2182 | Iterations: 25000 | Time: 334.212\n",
      "Loss: 0.2224 | Iterations: 30000 | Time: 401.040\n",
      "Loss: 0.2364 | Iterations: 35000 | Time: 468.251\n",
      "Loss: 0.2383 | Iterations: 40000 | Time: 534.959\n",
      "Loss: 0.2332 | Iterations: 45000 | Time: 601.426\n",
      "Loss: 0.2217 | Iterations: 50000 | Time: 668.866\n",
      "Training Accuracy: 0.7148\n",
      "Validation Accuracy: 0.5613\n",
      "Epoch: 5\n",
      "Loss: 0.2088 | Iterations: 5000 | Time: 61.202\n",
      "Loss: 0.2126 | Iterations: 10000 | Time: 128.025\n",
      "Loss: 0.2164 | Iterations: 15000 | Time: 194.723\n",
      "Loss: 0.2022 | Iterations: 20000 | Time: 261.727\n",
      "Loss: 0.1975 | Iterations: 25000 | Time: 328.946\n",
      "Loss: 0.1957 | Iterations: 30000 | Time: 395.927\n",
      "Loss: 0.1966 | Iterations: 35000 | Time: 462.463\n",
      "Loss: 0.2081 | Iterations: 40000 | Time: 529.562\n",
      "Loss: 0.2007 | Iterations: 45000 | Time: 596.001\n",
      "Loss: 0.2175 | Iterations: 50000 | Time: 662.558\n",
      "Training Accuracy: 0.7400\n",
      "Validation Accuracy: 0.5750\n",
      "Epoch: 6\n",
      "Loss: 0.1759 | Iterations: 5000 | Time: 57.300\n",
      "Loss: 0.1905 | Iterations: 10000 | Time: 121.092\n",
      "Loss: 0.2076 | Iterations: 15000 | Time: 187.633\n",
      "Loss: 0.1996 | Iterations: 20000 | Time: 254.679\n",
      "Loss: 0.1956 | Iterations: 25000 | Time: 321.445\n",
      "Loss: 0.2034 | Iterations: 30000 | Time: 388.768\n",
      "Loss: 0.2066 | Iterations: 35000 | Time: 455.294\n",
      "Loss: 0.1862 | Iterations: 40000 | Time: 522.066\n",
      "Loss: 0.1987 | Iterations: 45000 | Time: 588.929\n",
      "Loss: 0.1833 | Iterations: 50000 | Time: 655.429\n",
      "Training Accuracy: 0.7375\n",
      "Validation Accuracy: 0.5686\n",
      "Epoch: 7\n",
      "Loss: 0.1842 | Iterations: 5000 | Time: 57.633\n",
      "Loss: 0.1773 | Iterations: 10000 | Time: 114.840\n",
      "Loss: 0.1866 | Iterations: 15000 | Time: 179.871\n",
      "Loss: 0.1814 | Iterations: 20000 | Time: 246.668\n",
      "Loss: 0.1807 | Iterations: 25000 | Time: 313.485\n",
      "Loss: 0.1852 | Iterations: 30000 | Time: 380.468\n",
      "Loss: 0.2002 | Iterations: 35000 | Time: 447.203\n",
      "Loss: 0.1855 | Iterations: 40000 | Time: 514.282\n",
      "Loss: 0.1903 | Iterations: 45000 | Time: 581.008\n",
      "Loss: 0.1781 | Iterations: 50000 | Time: 647.715\n",
      "Training Accuracy: 0.6925\n",
      "Validation Accuracy: 0.5454\n",
      "Epoch: 8\n",
      "Loss: 0.1667 | Iterations: 5000 | Time: 57.280\n",
      "Loss: 0.1688 | Iterations: 10000 | Time: 114.847\n",
      "Loss: 0.1676 | Iterations: 15000 | Time: 172.508\n",
      "Loss: 0.1728 | Iterations: 20000 | Time: 235.667\n",
      "Loss: 0.1927 | Iterations: 25000 | Time: 302.228\n",
      "Loss: 0.1736 | Iterations: 30000 | Time: 368.577\n",
      "Loss: 0.1796 | Iterations: 35000 | Time: 435.202\n",
      "Loss: 0.1839 | Iterations: 40000 | Time: 501.643\n",
      "Loss: 0.1908 | Iterations: 45000 | Time: 567.932\n",
      "Loss: 0.1764 | Iterations: 50000 | Time: 634.379\n",
      "Training Accuracy: 0.7550\n",
      "Validation Accuracy: 0.5693\n",
      "Epoch: 9\n",
      "Loss: 0.1709 | Iterations: 5000 | Time: 57.348\n",
      "Loss: 0.1635 | Iterations: 10000 | Time: 114.492\n",
      "Loss: 0.1597 | Iterations: 15000 | Time: 171.659\n",
      "Loss: 0.1722 | Iterations: 20000 | Time: 229.521\n",
      "Loss: 0.1756 | Iterations: 25000 | Time: 286.406\n",
      "Loss: 0.1644 | Iterations: 30000 | Time: 347.886\n",
      "Loss: 0.1745 | Iterations: 35000 | Time: 414.584\n",
      "Loss: 0.1702 | Iterations: 40000 | Time: 480.699\n",
      "Loss: 0.1673 | Iterations: 45000 | Time: 547.167\n",
      "Loss: 0.1523 | Iterations: 50000 | Time: 614.222\n",
      "Training Accuracy: 0.7298\n",
      "Validation Accuracy: 0.5618\n",
      "Epoch: 10\n",
      "Loss: 0.1506 | Iterations: 5000 | Time: 57.066\n",
      "Loss: 0.1433 | Iterations: 10000 | Time: 114.423\n",
      "Loss: 0.1414 | Iterations: 15000 | Time: 171.807\n",
      "Loss: 0.1650 | Iterations: 20000 | Time: 229.221\n",
      "Loss: 0.1900 | Iterations: 25000 | Time: 286.808\n",
      "Loss: 0.1680 | Iterations: 30000 | Time: 345.707\n",
      "Loss: 0.1564 | Iterations: 35000 | Time: 412.395\n",
      "Loss: 0.1569 | Iterations: 40000 | Time: 479.171\n",
      "Loss: 0.1846 | Iterations: 45000 | Time: 545.654\n",
      "Loss: 0.1768 | Iterations: 50000 | Time: 612.229\n",
      "Training Accuracy: 0.7347\n",
      "Validation Accuracy: 0.5559\n",
      "Test Accuracy: 0.4495\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_accuracy</td><td>▁▅▆▆▇▇▆█▇▇</td></tr><tr><td>training_loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▅▇▇██▆█▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.44946</td></tr><tr><td>training_accuracy</td><td>0.73471</td></tr><tr><td>training_loss</td><td>0.16399</td></tr><tr><td>validation_accuracy</td><td>0.55591</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">light-sweep-16</strong> at: <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/uwqzss76' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/uwqzss76</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_074604-uwqzss76/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 21sikst0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-05\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bersi/Documents/CS6910 Assignments/CS6910-Assignment3/wandb/run-20230508_105029-21sikst0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/21sikst0' target=\"_blank\">peachy-sweep-17</a></strong> to <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/21sikst0' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/21sikst0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 2.1587 | Iterations: 5000 | Time: 50.854\n",
      "Loss: 1.6599 | Iterations: 10000 | Time: 106.155\n",
      "Loss: 3.3518 | Iterations: 15000 | Time: 156.491\n",
      "Loss: 3.2723 | Iterations: 20000 | Time: 207.549\n",
      "Loss: 3.1889 | Iterations: 25000 | Time: 262.611\n",
      "Loss: 3.0990 | Iterations: 30000 | Time: 318.478\n",
      "Loss: 3.0760 | Iterations: 35000 | Time: 373.213\n",
      "Loss: 3.1313 | Iterations: 40000 | Time: 428.054\n",
      "Loss: 2.9977 | Iterations: 45000 | Time: 483.086\n",
      "Loss: 2.9157 | Iterations: 50000 | Time: 537.852\n",
      "Training Accuracy: 0.0000\n",
      "Validation Accuracy: 0.0000\n",
      "Epoch: 2\n",
      "Loss: 2.8641 | Iterations: 5000 | Time: 49.723\n",
      "Loss: 2.8685 | Iterations: 10000 | Time: 100.967\n",
      "Loss: 2.8480 | Iterations: 15000 | Time: 152.627\n",
      "Loss: 2.8868 | Iterations: 20000 | Time: 204.608\n",
      "Loss: 2.9156 | Iterations: 25000 | Time: 256.434\n",
      "Loss: 2.8850 | Iterations: 30000 | Time: 306.015\n",
      "Loss: 2.8736 | Iterations: 35000 | Time: 353.528\n",
      "Loss: 2.8747 | Iterations: 40000 | Time: 400.450\n",
      "Loss: 2.8233 | Iterations: 45000 | Time: 447.194\n",
      "Loss: 2.8754 | Iterations: 50000 | Time: 494.619\n",
      "Training Accuracy: 0.0000\n",
      "Validation Accuracy: 0.0000\n",
      "Test Accuracy: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_accuracy</td><td>▁▁</td></tr><tr><td>training_loss</td><td>█▁</td></tr><tr><td>validation_accuracy</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>test_accuracy</td><td>0.0</td></tr><tr><td>training_accuracy</td><td>0.0</td></tr><tr><td>training_loss</td><td>2.8707</td></tr><tr><td>validation_accuracy</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-sweep-17</strong> at: <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/21sikst0' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/21sikst0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_105029-21sikst0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ran2b014 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bersi/Documents/CS6910 Assignments/CS6910-Assignment3/wandb/run-20230508_112932-ran2b014</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/ran2b014' target=\"_blank\">lively-sweep-18</a></strong> to <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/ran2b014' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/ran2b014</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 2.6564 | Iterations: 5000 | Time: 59.430\n",
      "Loss: 2.4868 | Iterations: 10000 | Time: 119.458\n",
      "Loss: 1.8959 | Iterations: 15000 | Time: 180.046\n",
      "Loss: 1.1869 | Iterations: 20000 | Time: 240.960\n",
      "Loss: 0.7729 | Iterations: 25000 | Time: 302.499\n",
      "Loss: 0.5683 | Iterations: 30000 | Time: 363.279\n",
      "Loss: 0.5053 | Iterations: 35000 | Time: 419.885\n",
      "Loss: 0.4319 | Iterations: 40000 | Time: 476.488\n",
      "Loss: 0.4050 | Iterations: 45000 | Time: 533.522\n",
      "Loss: 0.3644 | Iterations: 50000 | Time: 590.581\n",
      "Training Accuracy: 0.5813\n",
      "Validation Accuracy: 0.4741\n",
      "Epoch: 2\n",
      "Loss: 0.3499 | Iterations: 5000 | Time: 61.951\n",
      "Loss: 0.3307 | Iterations: 10000 | Time: 124.347\n",
      "Loss: 0.3194 | Iterations: 15000 | Time: 186.559\n",
      "Loss: 0.3047 | Iterations: 20000 | Time: 248.370\n",
      "Loss: 0.2872 | Iterations: 25000 | Time: 310.266\n",
      "Loss: 0.2904 | Iterations: 30000 | Time: 378.807\n",
      "Loss: 0.3101 | Iterations: 35000 | Time: 445.906\n",
      "Loss: 0.2846 | Iterations: 40000 | Time: 509.668\n",
      "Loss: 0.2888 | Iterations: 45000 | Time: 572.429\n",
      "Loss: 0.2536 | Iterations: 50000 | Time: 633.816\n",
      "Training Accuracy: 0.6685\n",
      "Validation Accuracy: 0.5249\n",
      "Epoch: 3\n",
      "Loss: 0.2501 | Iterations: 5000 | Time: 66.844\n",
      "Loss: 0.2505 | Iterations: 10000 | Time: 133.097\n",
      "Loss: 0.2433 | Iterations: 15000 | Time: 198.876\n",
      "Loss: 0.2379 | Iterations: 20000 | Time: 265.072\n",
      "Loss: 0.2489 | Iterations: 25000 | Time: 330.642\n",
      "Loss: 0.2587 | Iterations: 30000 | Time: 397.865\n",
      "Loss: 0.2504 | Iterations: 35000 | Time: 465.150\n",
      "Loss: 0.2422 | Iterations: 40000 | Time: 525.428\n",
      "Loss: 0.2452 | Iterations: 45000 | Time: 585.233\n",
      "Loss: 0.2664 | Iterations: 50000 | Time: 645.777\n",
      "Training Accuracy: 0.6912\n",
      "Validation Accuracy: 0.5496\n",
      "Epoch: 4\n",
      "Loss: 0.2334 | Iterations: 5000 | Time: 67.204\n",
      "Loss: 0.2220 | Iterations: 10000 | Time: 132.569\n",
      "Loss: 0.2171 | Iterations: 15000 | Time: 197.704\n",
      "Loss: 0.2279 | Iterations: 20000 | Time: 263.746\n",
      "Loss: 0.2149 | Iterations: 25000 | Time: 330.877\n",
      "Loss: 0.2237 | Iterations: 30000 | Time: 396.676\n",
      "Loss: 0.2302 | Iterations: 35000 | Time: 462.508\n",
      "Loss: 0.2208 | Iterations: 40000 | Time: 526.857\n",
      "Loss: 0.2125 | Iterations: 45000 | Time: 588.885\n",
      "Loss: 0.2293 | Iterations: 50000 | Time: 652.091\n",
      "Training Accuracy: 0.7235\n",
      "Validation Accuracy: 0.5625\n",
      "Epoch: 5\n",
      "Loss: 0.1887 | Iterations: 5000 | Time: 65.709\n",
      "Loss: 0.1906 | Iterations: 10000 | Time: 131.029\n",
      "Loss: 0.2062 | Iterations: 15000 | Time: 196.833\n",
      "Loss: 0.1966 | Iterations: 20000 | Time: 261.741\n",
      "Loss: 0.2118 | Iterations: 25000 | Time: 326.955\n",
      "Loss: 0.1988 | Iterations: 30000 | Time: 392.330\n",
      "Loss: 0.1989 | Iterations: 35000 | Time: 457.882\n",
      "Loss: 0.2114 | Iterations: 40000 | Time: 523.444\n",
      "Loss: 0.1991 | Iterations: 45000 | Time: 584.069\n",
      "Loss: 0.1991 | Iterations: 50000 | Time: 644.156\n",
      "Training Accuracy: 0.7176\n",
      "Validation Accuracy: 0.5623\n",
      "Epoch: 6\n",
      "Loss: 0.2169 | Iterations: 5000 | Time: 64.977\n",
      "Loss: 0.1892 | Iterations: 10000 | Time: 130.301\n",
      "Loss: 0.1937 | Iterations: 15000 | Time: 195.483\n",
      "Loss: 0.1866 | Iterations: 20000 | Time: 260.704\n",
      "Loss: 0.1882 | Iterations: 25000 | Time: 326.007\n",
      "Loss: 0.2009 | Iterations: 30000 | Time: 391.212\n",
      "Loss: 0.1918 | Iterations: 35000 | Time: 456.961\n",
      "Loss: 0.1943 | Iterations: 40000 | Time: 522.307\n",
      "Loss: 0.1987 | Iterations: 45000 | Time: 584.369\n",
      "Loss: 0.1872 | Iterations: 50000 | Time: 644.272\n",
      "Training Accuracy: 0.7421\n",
      "Validation Accuracy: 0.5688\n",
      "Epoch: 7\n",
      "Loss: 0.1868 | Iterations: 5000 | Time: 65.639\n",
      "Loss: 0.1897 | Iterations: 10000 | Time: 131.414\n",
      "Loss: 0.1668 | Iterations: 15000 | Time: 196.914\n",
      "Loss: 0.1824 | Iterations: 20000 | Time: 262.411\n",
      "Loss: 0.1936 | Iterations: 25000 | Time: 327.364\n",
      "Loss: 0.1913 | Iterations: 30000 | Time: 393.102\n",
      "Loss: 0.1718 | Iterations: 35000 | Time: 458.508\n",
      "Loss: 0.1711 | Iterations: 40000 | Time: 523.914\n",
      "Loss: 0.1830 | Iterations: 45000 | Time: 587.398\n",
      "Loss: 0.1799 | Iterations: 50000 | Time: 646.822\n",
      "Training Accuracy: 0.7586\n",
      "Validation Accuracy: 0.5784\n",
      "Epoch: 8\n",
      "Loss: 0.1532 | Iterations: 5000 | Time: 65.474\n",
      "Loss: 0.1689 | Iterations: 10000 | Time: 130.717\n",
      "Loss: 0.1712 | Iterations: 15000 | Time: 196.140\n",
      "Loss: 0.1551 | Iterations: 20000 | Time: 261.677\n",
      "Loss: 0.1746 | Iterations: 25000 | Time: 326.885\n",
      "Loss: 0.1804 | Iterations: 30000 | Time: 391.897\n",
      "Loss: 0.1791 | Iterations: 35000 | Time: 456.579\n",
      "Loss: 0.1766 | Iterations: 40000 | Time: 522.017\n",
      "Loss: 0.1614 | Iterations: 45000 | Time: 587.125\n",
      "Loss: 0.1723 | Iterations: 50000 | Time: 647.015\n",
      "Training Accuracy: 0.7571\n",
      "Validation Accuracy: 0.5703\n",
      "Epoch: 9\n",
      "Loss: 0.1562 | Iterations: 5000 | Time: 65.539\n",
      "Loss: 0.1605 | Iterations: 10000 | Time: 130.830\n",
      "Loss: 0.1668 | Iterations: 15000 | Time: 196.553\n",
      "Loss: 0.1630 | Iterations: 20000 | Time: 262.570\n",
      "Loss: 0.1648 | Iterations: 25000 | Time: 328.327\n",
      "Loss: 0.1523 | Iterations: 30000 | Time: 393.566\n",
      "Loss: 0.1725 | Iterations: 35000 | Time: 458.695\n",
      "Loss: 0.1617 | Iterations: 40000 | Time: 523.925\n",
      "Loss: 0.1602 | Iterations: 45000 | Time: 589.306\n",
      "Loss: 0.1747 | Iterations: 50000 | Time: 650.678\n",
      "Training Accuracy: 0.7634\n",
      "Validation Accuracy: 0.5681\n",
      "Epoch: 10\n",
      "Loss: 0.1399 | Iterations: 5000 | Time: 58.871\n",
      "Loss: 0.1436 | Iterations: 10000 | Time: 129.762\n",
      "Loss: 0.1530 | Iterations: 15000 | Time: 199.375\n",
      "Loss: 0.1620 | Iterations: 20000 | Time: 269.120\n",
      "Loss: 0.1697 | Iterations: 25000 | Time: 340.202\n",
      "Loss: 0.1809 | Iterations: 30000 | Time: 408.929\n",
      "Loss: 0.1781 | Iterations: 35000 | Time: 477.771\n",
      "Loss: 0.1633 | Iterations: 40000 | Time: 545.532\n",
      "Loss: 0.1624 | Iterations: 45000 | Time: 615.664\n",
      "Loss: 0.1721 | Iterations: 50000 | Time: 688.309\n",
      "Training Accuracy: 0.7631\n",
      "Validation Accuracy: 0.5813\n",
      "Test Accuracy: 0.4678\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>training_accuracy</td><td>▁▄▅▆▆▇████</td></tr><tr><td>training_loss</td><td>█▂▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▄▆▇▇▇█▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>test_accuracy</td><td>0.46777</td></tr><tr><td>training_accuracy</td><td>0.76309</td></tr><tr><td>training_loss</td><td>0.16239</td></tr><tr><td>validation_accuracy</td><td>0.5813</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lively-sweep-18</strong> at: <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/ran2b014' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/ran2b014</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230508_112932-ran2b014/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 47dndcjc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bersi/Documents/CS6910 Assignments/CS6910-Assignment3/wandb/run-20230508_143100-47dndcjc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/47dndcjc' target=\"_blank\">eager-sweep-19</a></strong> to <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/sweeps/w613tuxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/47dndcjc' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/CS6910_Assn3_Attention_RNN/runs/47dndcjc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 2.6571 | Iterations: 5000 | Time: 66.352\n",
      "Loss: 2.4616 | Iterations: 10000 | Time: 136.722\n",
      "Loss: 1.8998 | Iterations: 15000 | Time: 203.734\n",
      "Loss: 1.2988 | Iterations: 20000 | Time: 271.772\n",
      "Loss: 0.8962 | Iterations: 25000 | Time: 340.954\n",
      "Loss: 0.6903 | Iterations: 30000 | Time: 408.761\n",
      "Loss: 0.5751 | Iterations: 35000 | Time: 478.017\n",
      "Loss: 0.5021 | Iterations: 40000 | Time: 546.549\n",
      "Loss: 0.4569 | Iterations: 45000 | Time: 615.391\n",
      "Loss: 0.4546 | Iterations: 50000 | Time: 676.836\n",
      "Training Accuracy: 0.5400\n",
      "Validation Accuracy: 0.4558\n",
      "Epoch: 2\n",
      "Loss: 0.3608 | Iterations: 5000 | Time: 72.526\n",
      "Loss: 0.3635 | Iterations: 10000 | Time: 150.437\n",
      "Loss: 0.3567 | Iterations: 15000 | Time: 227.718\n",
      "Loss: 0.3390 | Iterations: 20000 | Time: 305.388\n",
      "Loss: 0.3412 | Iterations: 25000 | Time: 382.777\n",
      "Loss: 0.3382 | Iterations: 30000 | Time: 460.133\n",
      "Loss: 0.3168 | Iterations: 35000 | Time: 538.860\n",
      "Loss: 0.3279 | Iterations: 40000 | Time: 617.424\n",
      "Loss: 0.3129 | Iterations: 45000 | Time: 695.390\n",
      "Loss: 0.3096 | Iterations: 50000 | Time: 773.713\n",
      "Training Accuracy: 0.6416\n",
      "Validation Accuracy: 0.5154\n",
      "Epoch: 3\n",
      "Loss: 0.2707 | Iterations: 5000 | Time: 62.298\n",
      "Loss: 0.2570 | Iterations: 10000 | Time: 130.186\n"
     ]
    }
   ],
   "source": [
    "sweep_id = \"w613tuxy\"\n",
    "wandb.agent(sweep_id, train_sweep, count=10, project=\"CS6910_Assn3_Attention_RNN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
